{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2193909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lemai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "from datasets import DatasetDict, Dataset\n",
    "import nbimporter\n",
    "from utlis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4581693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_special_keys(review):\n",
    "    special_character = re.compile(\"ï¿½+\")\n",
    "    return special_character.sub(r'', review)\n",
    "\n",
    "def rm_punctuation(review):\n",
    "    punctuation = re.compile(r\"[!#$%&()*+;<=>?@[\\]^_`{|}~]+\")\n",
    "    return punctuation.sub(r\"\", review)\n",
    "\n",
    "def rm_emoji(review):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001F004-\\U0001F0CF\"  # Mahjong Tiles\n",
    "        u\"\\U0001F170-\\U0001F251\"  # Enclosed Characters\n",
    "        u\"\\U0001F300-\\U0001F9F9\"  # Additional symbols and emojis\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', review)\n",
    "    return text\n",
    "\n",
    "def rm_urls_paths(text):\n",
    "    # Define a regex pattern to match both URLs and file paths\n",
    "    url_pattern = r'https?[:]//\\S+|www\\.\\S+'\n",
    "    path_pattern = r'(?:(?:[a-z]:\\\\|\\\\\\\\|/)[^\\s|/]+(?:/[^\\s|/]+)*)'\n",
    "    combined_pattern = f'({url_pattern})|({path_pattern})'\n",
    "    cleaned_text = re.sub(combined_pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_annotatation(text):\n",
    "    khach_san = \"\\bkhach san ?|\\bksan ?|\\bks ?\"\n",
    "    return re.sub(\"\\bnv ?\", \"nhÃ¢n viÃªn\",re.sub(khach_san, \"khÃ¡ch sáº¡n\", text))\n",
    "\n",
    "def rm_escape_characters(text):\n",
    "    cleaned_text = text.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace('\\q', '').replace('\\w', '').replace('\\s', '')\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(review):\n",
    "    cleaned_review = {\"Review\": rm_escape_characters(normalize_annotatation(rm_special_keys(rm_punctuation(rm_emoji(rm_urls_paths(review['Review'].lower()))))))}\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d6052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess():\n",
    "    def __init__(self):\n",
    "        self.proj_path = get_proj_path()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.segmenter = VnCoreNLP(os.path.join(self.proj_path, 'vncorenlp', 'VnCoreNLP-1.1.1.jar'), annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "        self.feature = ['giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'di_chuyen', 'mua_sam']\n",
    "        \n",
    "    def segment(self, df):\n",
    "        return {\"Segment\": \" \".join([\" \".join(sen) for sen in self.segmenter.tokenize(df[\"Review\"])])}\n",
    "        \n",
    "    def tokenize(self, df):\n",
    "        return self.tokenizer(df[\"Segment\"], truncation=True, padding=True, max_length=165)\n",
    "    \n",
    "    def label(self, example):\n",
    "        return {'labels_regressor': np.array([example[i] for i in self.feature]),\n",
    "            'labels_classifier': np.array([int(example[i] != 0) for i in self.feature])}\n",
    "    \n",
    "    def rm_stopwords(self, text, remove_stopwords=True):\n",
    "        stopword_path = os.path.join(self.proj_path, \"vn_stopwords\", \"vietnamese-stopwords-dash.txt\")\n",
    "        with open(stopword_path, 'r', encoding='utf-8') as file:\n",
    "            stop_words = set(file.read().splitlines())    \n",
    "        words = text['Review'].split()\n",
    "        if remove_stopwords:\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "        cleaned_text = ' '.join(words)\n",
    "        return {\"Review\": cleaned_text}\n",
    "        \n",
    "    def run(self, dataset):\n",
    "        dataset = dataset.map(clean_text)\n",
    "        dataset = dataset.map(self.segment)\n",
    "        dataset = dataset.map(self.tokenize, batched=True)\n",
    "        dataset = dataset.map(self.label)\n",
    "        dataset = dataset.map(self.rm_stopwords)\n",
    "        dataset.set_format(\"torch\")\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16b62bb1-f182-4eb0-936e-a8059898121a",
   "metadata": {},
   "source": [
    "data_path = r\"D:\\FSoft\\Review_Ana\\Dream_Tim\\A\\datasets\\data_original\\Original-datasets.csv\"\n",
    "train_df = pd.read_csv(data_path)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eab35cb0-7495-4679-8273-08dc90d629cf",
   "metadata": {},
   "source": [
    "new_value = 'TÃ´i báº¯t xe 7ï¿½ chá»— Ä‘i tá»« sÃ¢n bay vá» nhÃ .ThÃ¡i Ä‘á»™ cá»§a tÃ iï¿½ xáº¿ khÃ´ngï¿½ vui váº» khi Ä‘Ã³n chÃºng tÃ´i.máº·t thÃ¬ nhÄƒn nhÃ³ thÃ¡i Ä‘á»™ thÃ¬ lÆ¡ lÆ¡.gia Ä‘Ã¬nh Ä‘i \\r\\n7 ngÆ°á»i.tÃ i xe má»Ÿ cá»‘p xe rá»“i Ä‘á»ƒ tÃ´i tá»± xáº¿p hÃ nh lÃ½ vÃ o.sau Ä‘Ã³ dáº¹p lun 2 gháº¿ sau Ä‘á»ƒ cháº¥t vali lÃªn.5 ngÆ°á»i!!!@@ trong gia Ä‘Ã¬nh pháº£i dá»“n vÃ´ ngá»“i gháº¿ giá»¯a. 2 ngÆ°á»i ngá»“i gháº¿ trÆ°á»›c.lÃªn xe thÃ¬ nÃ³ng.tÃ´i yÃªu cáº§u tÃ i xáº¿%^& ðŸ˜‚ má»Ÿ mÃ¡y láº¡nh thÃ¬ tÃ i xáº¿ báº£o cáº£ sÃ¡ng h()#% Ä‘áº­u ngoÃ i náº¯ng nÃªn nÃ³ng.cháº¡y\\r\\n 10p váº«n chÆ°a tháº¥y má»Ÿ mÃ¡y láº¡nh.mÃ #&^#&ðŸ˜‚ðŸ˜‚ trong xe nÃ³ng nhÆ° cÃ¡i lÃ² 5 ngÆ°á»i ngá»“i chen nhau.há»i tiáº¿p thÃ¬ khÃ´ng tráº£ lá»i.sau Ä‘Ã³ mÃ¬nh yÃªu cáº§u nhiá»u quÃ¡ má»›i kÃªu Ä‘ang má»Ÿ.vá» gáº§n Ä‘áº¿n nhÃ  má»›i tháº¥y quáº¡t nÃ³ thá»•i mÃ¡t Ä‘Æ°á»£c xÃ­u.ngá»“i trÃªn xe 30p mÃ  nhÆ° cá»±c hÃ¬nh.yÃªu cáº§u cÃ´ng ty xem xÃ©t láº¡i thÃ¡i Ä‘á»™ lÃ m viá»‡c cá»§a tÃ i xáº¿ cháº¡y xe 6898 lÃºc 10h sÃ¡ng ngÃ y 10 thÃ¡ng 7.nghiÃªm tÃºc phÃª bÃ¬nh.https://example.com or visit C:\\\\Documents\\\\file.txt. hoáº·c lÃ  www.example.com.vn'\n",
    "\n",
    "# Äáº·t giÃ¡ trá»‹ má»›i cho hÃ ng vÃ  cá»™t cá»¥ thá»ƒ trong DataFrame\n",
    "train_df.at[7, 'Review']=new_value\n",
    "train_df.at[7, 'Review']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c74ec9b-4cb7-49c6-aeb3-112394bc73fa",
   "metadata": {},
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Táº¡o má»™t DatasetDict má»›i\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset\n",
    "})\n",
    "dataset_dict['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "847a3299-5b4e-4b2c-98ea-088d45c7bc92",
   "metadata": {},
   "source": [
    "reviews_df = dataset_dict.copy()\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb17403-614e-493d-af49-d52481b6bb37",
   "metadata": {},
   "source": [
    "prep = preprocess()\n",
    "tokenized_datasets = prep.run(dataset_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe7ee4d1-79c7-4677-8ce7-4f472a499437",
   "metadata": {},
   "source": [
    "reviews_df['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "965cdcd8-f690-4d44-acdf-bd6243571abb",
   "metadata": {},
   "source": [
    "tokenized_datasets['train']['Review'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e30ecaf-59ac-4f4b-a76c-ddbfb385efc9",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f46873ee-f086-477e-8013-8233f72bcdd0",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05da88-ab50-479d-a87a-71f63278685d",
   "metadata": {},
   "source": [
    "df.iloc[0].input_ids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40636a79-9667-4af7-8f2f-8edd22bf0fb3",
   "metadata": {},
   "source": [
    "review=1\n",
    "for i in range(len(df.iloc[review].input_ids)):\n",
    "    print(f'{df.iloc[review].input_ids[i]} ---> {prep.tokenizer.decode(df.iloc[0].input_ids[i])}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f9fbd44-6baa-4c90-b6a6-52bff26164fe",
   "metadata": {},
   "source": [
    "df['labels_regressor'][3687]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "060c4b2f-49f2-4fdb-8b35-b4058fba5488",
   "metadata": {},
   "source": [
    "df['labels_classifier'][3687]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
