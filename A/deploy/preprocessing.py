import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import os
from transformers import AutoTokenizer
from vncorenlp import VnCoreNLP
from datasets import load_dataset, DatasetDict, Dataset

#-----------------------------------------------------------------------------------------------------------------------

def rm_special_keys(review):
    special_character = re.compile("ï¿½+")
    return special_character.sub(r'', review)

def rm_punctuation(review):
    punctuation = re.compile(r"[!#$%&()*+;<=>?@[\]^_`{|}~]+")
    return punctuation.sub(r"", review)

def rm_emoji(review):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U0001F300-\U0001F5FF"  # Symbols & Pictographs
        u"\U0001F680-\U0001F6FF"  # Transport & Map Symbols
        u"\U0001F700-\U0001F77F"  # Alchemical Symbols
        u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
        u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
        u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        u"\U0001FA00-\U0001FA6F"  # Chess Symbols
        u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        u"\U0001F004-\U0001F0CF"  # Mahjong Tiles
        u"\U0001F170-\U0001F251"  # Enclosed Characters
        u"\U0001F300-\U0001F9F9"  # Additional symbols and emojis
        u"\U00002702-\U000027B0"  # Dingbats
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', review)
    return text

def rm_urls_paths(text):
    # Define a regex pattern to match both URLs and file paths
    url_pattern = r'https?[:]//\S+|www\.\S+'
    path_pattern = r'(?:(?:[a-z]:\\|\\\\|/)[^\s|/]+(?:/[^\s|/]+)*)'
    combined_pattern = f'({url_pattern})|({path_pattern})'
    cleaned_text = re.sub(combined_pattern, '', text)
    return cleaned_text

def normalize_annotatation(text):
    khach_san = "\bkhach san ?|\bksan ?|\bks ?"
    return re.sub("\bnv ?", "nhÃ¢n viÃªn",re.sub(khach_san, "khÃ¡ch sáº¡n", text))

def rm_escape_characters(text):
    cleaned_text = text.replace('\r', '').replace('\n', '').replace('\t', '').replace('\q', '').replace('\w', '').replace('\s', '')
    return cleaned_text

def clean_text(review):
    cleaned_review = {"Review": rm_escape_characters(normalize_annotatation(rm_special_keys(rm_punctuation(rm_emoji(rm_urls_paths(review['Review'].lower()))))))}
    return cleaned_review

#-----------------------------------------------------------------------------------------------------------------------

class preprocess():
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
        self.segmenter = VnCoreNLP(r"D:\FSoft\Review_Ana\Dream_Tim\A\vncorenlp\VnCoreNLP-1.1.1.jar", annotators="wseg", max_heap_size='-Xmx500m')
        self.feature = ['giai_tri', 'luu_tru', 'nha_hang', 'an_uong', 'di_chuyen', 'mua_sam']
        
    def segment(self, df):
        return {"Segment": " ".join([" ".join(sen) for sen in self.segmenter.tokenize(df["Review"])])}
        
    def tokenize(self, df):
        return self.tokenizer(df["Segment"], truncation=True, padding=True, max_length=165)
    
    def label(self, example):
        return {'labels_regressor': np.array([example[i] for i in self.feature]),
            'labels_classifier': np.array([int(example[i] != 0) for i in self.feature])}
    
    def rm_stopwords(self, text, remove_stopwords=True):
        dir_path = r"D:\FSoft\Review_Ana\Dream_Tim\A"
        stopword_path = os.path.join(dir_path, r"vn_stopwords\vietnamese-stopwords-dash.txt")
        with open(stopword_path, 'r', encoding='utf-8') as file:
            stop_words = set(file.read().splitlines())    
        words = text['Review'].split()
        if remove_stopwords:
            words = [word for word in words if word.lower() not in stop_words]
        cleaned_text = ' '.join(words)
        return {"Review": cleaned_text}
        
    def run(self, dataset):
        dataset = dataset.map(clean_text)
        dataset = dataset.map(self.segment)
        dataset = dataset.map(self.tokenize, batched=True)
        dataset = dataset.map(self.label)
        dataset = dataset.map(self.rm_stopwords)
        dataset.set_format("torch")
        
        return dataset

#-----------------------------------------------------------------------------------------------------------------------

if __name__ == "__main__":
    data_path = r"D:\FSoft\Review_Ana\Dream_Tim\A\datasets\data_original\Original-datasets.csv"
    train_df = pd.read_csv(data_path)
    train_df

    # Change value in roder to see the changes
    new_value = 'TÃ´i báº¯t xe 7ï¿½ chá»— Ä‘i tá»« sÃ¢n bay vá» nhÃ .ThÃ¡i Ä‘á»™ cá»§a tÃ iï¿½ xáº¿ khÃ´ngï¿½ vui váº» khi Ä‘Ã³n chÃºng tÃ´i.máº·t thÃ¬ nhÄƒn nhÃ³ thÃ¡i Ä‘á»™ thÃ¬ lÆ¡ lÆ¡.gia Ä‘Ã¬nh Ä‘i \r\n7 ngÆ°á»i.tÃ i xe má»Ÿ cá»‘p xe rá»“i Ä‘á»ƒ tÃ´i tá»± xáº¿p hÃ nh lÃ½ vÃ o.sau Ä‘Ã³ dáº¹p lun 2 gháº¿ sau Ä‘á»ƒ cháº¥t vali lÃªn.5 ngÆ°á»i!!!@@ trong gia Ä‘Ã¬nh pháº£i dá»“n vÃ´ ngá»“i gháº¿ giá»¯a. 2 ngÆ°á»i ngá»“i gháº¿ trÆ°á»›c.lÃªn xe thÃ¬ nÃ³ng.tÃ´i yÃªu cáº§u tÃ i xáº¿%^& ğŸ˜‚ má»Ÿ mÃ¡y láº¡nh thÃ¬ tÃ i xáº¿ báº£o cáº£ sÃ¡ng h()#% Ä‘áº­u ngoÃ i náº¯ng nÃªn nÃ³ng.cháº¡y\r\n 10p váº«n chÆ°a tháº¥y má»Ÿ mÃ¡y láº¡nh.mÃ #&^#&ğŸ˜‚ğŸ˜‚ trong xe nÃ³ng nhÆ° cÃ¡i lÃ² 5 ngÆ°á»i ngá»“i chen nhau.há»i tiáº¿p thÃ¬ khÃ´ng tráº£ lá»i.sau Ä‘Ã³ mÃ¬nh yÃªu cáº§u nhiá»u quÃ¡ má»›i kÃªu Ä‘ang má»Ÿ.vá» gáº§n Ä‘áº¿n nhÃ  má»›i tháº¥y quáº¡t nÃ³ thá»•i mÃ¡t Ä‘Æ°á»£c xÃ­u.ngá»“i trÃªn xe 30p mÃ  nhÆ° cá»±c hÃ¬nh.yÃªu cáº§u cÃ´ng ty xem xÃ©t láº¡i thÃ¡i Ä‘á»™ lÃ m viá»‡c cá»§a tÃ i xáº¿ cháº¡y xe 6898 lÃºc 10h sÃ¡ng ngÃ y 10 thÃ¡ng 7.nghiÃªm tÃºc phÃª bÃ¬nh.https://example.com or visit C:\\Documents\\file.txt. hoáº·c lÃ  www.example.com.vn'
    train_df.at[7, 'Review']=new_value
    train_df.at[7, 'Review']

    # Convert dataset to DatasetDict()
    train_dataset = Dataset.from_pandas(train_df)
    dataset_dict = DatasetDict({
        'train': train_dataset
    })


    reviews_df = dataset_dict.copy()

    # PREPROCESS
    prep = preprocess()
    tokenized_datasets = prep.run(dataset_dict)

    # Compare result between original with preprocessing data
    reviews_df['train']['Review'][7]
    print(tokenized_datasets['train']['Review'][7])